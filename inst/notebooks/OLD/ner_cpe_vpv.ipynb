{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import torch\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7061, 0.3341, 0.8402],\n",
      "        [0.1260, 0.6928, 0.2567],\n",
      "        [0.3885, 0.8542, 0.7658],\n",
      "        [0.6511, 0.6868, 0.5815],\n",
      "        [0.7998, 0.4384, 0.0408]])\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is configured\n",
    "print(torch.rand(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/DEVEL/code/data/ner_trainsets/v1.0.0.9024/allo/cpes_rasa_vpv_2k.csv\"\n",
    "test_path = \"/DEVEL/code/data/ner_trainsets/v1.0.0.9024/allo/cpes_rasa_vpv_100.csv\"\n",
    "num_epochs = 10\n",
    "num_decay = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "with open(train_path) as csvfile:\n",
    "    raw_csv = csv.reader(csvfile, delimiter=',')\n",
    "    for row in raw_csv:\n",
    "        raw_text = raw_text + \"\\n\" + row[1]\n",
    "\n",
    "# remove last empty line\n",
    "train_text = raw_text.split(\"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text2 = \"\"\n",
    "with open(test_path) as csvfile:\n",
    "    raw_csv = csv.reader(csvfile, delimiter=',')\n",
    "    for row in raw_csv:\n",
    "        raw_text2 = raw_text2 + \"\\n\" + row[1]\n",
    "\n",
    "# remove last empty line\n",
    "test_text = raw_text2.split(\"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token)\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\"))\n",
    "\n",
    "    return tokens_with_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adobe', 'O'), ('acrobat', 'B-cpe_product'), ('x', 'O'), ('(10.1)', 'O')]\n",
      "[('red', 'O'), ('hat', 'O'), ('wildfly', 'B-cpe_product'), ('core', 'I-cpe_product'), ('2.0.0', 'O'), ('alpha', 'O'), ('2', 'O')]\n",
      "[('tumbleweed', 'B-cpe_vendor'), ('server', 'B-cpe_product'), ('validator', 'I-cpe_product'), ('suite', 'I-cpe_product'), ('4.10', 'B-cpe_version')]\n",
      "[('gfi', 'B-cpe_vendor'), ('archiver', 'B-cpe_product'), ('15.2', 'B-cpe_version')]\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens_with_entities(\"adobe [acrobat](cpe_product) x (10.1)\"))\n",
    "print(get_tokens_with_entities(\"red hat [wildfly core](cpe_product) 2.0.0 alpha 2\"))\n",
    "print(get_tokens_with_entities(test_text[0]))\n",
    "print(get_tokens_with_entities(test_text[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataMaker:\n",
    "    def __init__(self, texts):\n",
    "        self.unique_entities = []\n",
    "        self.processed_texts = []\n",
    "\n",
    "        temp_processed_texts = []\n",
    "        for text in texts:\n",
    "            tokens_with_entities = get_tokens_with_entities(text)\n",
    "            for _, ent in tokens_with_entities:\n",
    "                if ent not in self.unique_entities:\n",
    "                    self.unique_entities.append(ent)\n",
    "            temp_processed_texts.append(tokens_with_entities)\n",
    "\n",
    "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
    "\n",
    "        for tokens_with_entities in temp_processed_texts:\n",
    "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
    "\n",
    "    @property\n",
    "    def id2label(self):\n",
    "        return dict(enumerate(self.unique_entities))\n",
    "\n",
    "    @property\n",
    "    def label2id(self):\n",
    "        return {v:k for k, v in self.id2label.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
    "            ner_tags = []\n",
    "            tokens = []\n",
    "            for t, ent in tokens_with_encoded_entities:\n",
    "                ner_tags.append(ent)\n",
    "                tokens.append(t)\n",
    "\n",
    "            return {\n",
    "                \"id\": id,\n",
    "                \"ner_tags\": ner_tags,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
    "        if isinstance(idx, int):\n",
    "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
    "        else:\n",
    "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
    "\n",
    "    def as_hf_dataset(self, tokenizer):\n",
    "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        ids, ner_tags, tokens = [], [], []\n",
    "        for i, pt in enumerate(self.processed_texts):\n",
    "            ids.append(i)\n",
    "            pt_tokens,pt_tags = list(zip(*pt))\n",
    "            ner_tags.append(pt_tags)\n",
    "            tokens.append(pt_tokens)\n",
    "        data = {\n",
    "            \"id\": ids,\n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        features = Features({\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=dm.unique_entities)),\n",
    "            \"id\": Value(\"int32\")\n",
    "        })\n",
    "        ds = Dataset.from_dict(data, features)\n",
    "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "        return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples = 2000\n",
      "[{'id': 0, 'ner_tags': [2, 1, 3, 0, 0, 0], 'tokens': ['suse', 'rancher', '1.2.2', 'release', 'candidate', '3']}, {'id': 1, 'ner_tags': [2, 1, 3, 0, 0], 'tokens': ['extensis', 'mrsid', '1.99', 'for', 'irfanview']}, {'id': 2, 'ner_tags': [2, 1, 3], 'tokens': ['facebook', 'folly', '2018.05.14.00']}]\n"
     ]
    }
   ],
   "source": [
    "# Create Training NER Data Object\n",
    "dm = NERDataMaker(train_text)\n",
    "print(f\"total examples = {len(dm)}\")\n",
    "print(dm[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples = 100\n",
      "[{'id': 0, 'ner_tags': [2, 1, 4, 4, 3], 'tokens': ['tumbleweed', 'server', 'validator', 'suite', '4.10']}, {'id': 1, 'ner_tags': [2, 1, 3], 'tokens': ['gfi', 'archiver', '15.2']}, {'id': 2, 'ner_tags': [2, 1, 3], 'tokens': ['apperta', 'openeyes', '1.5.5']}]\n"
     ]
    }
   ],
   "source": [
    "# Create NER Data Object\n",
    "dm_test = NERDataMaker(test_text)\n",
    "print(f\"total examples = {len(dm_test)}\")\n",
    "print(dm_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "For this demo, Iâ€™ll use distilbert-base-uncased model. The dm object contains few properties which we pass to the AutoModelForTokenClassification.from_pretrained method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1564b291d85a4da18a5611c1c74987ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe08fd06e1d49afbe6e6cb9459b0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n",
    "test_ds = dm_test.as_hf_dataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "c:\\DEVEL\\software\\conda\\envs\\ner_poc\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf95744256b2498d8197bf828aca270c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06502786c61741cda5be9f8f13e8e28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11310423165559769, 'eval_runtime': 0.5156, 'eval_samples_per_second': 193.942, 'eval_steps_per_second': 13.576, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c940a7c82a4211a3c0d925b02c3a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06771220266819, 'eval_runtime': 0.5148, 'eval_samples_per_second': 194.245, 'eval_steps_per_second': 13.597, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea908455b314d6eaa82a1ec9b5353f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.062498826533555984, 'eval_runtime': 0.508, 'eval_samples_per_second': 196.858, 'eval_steps_per_second': 13.78, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.153, 'learning_rate': 1.2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f13e40dde440b80c40ee7fc72d551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.034930676221847534, 'eval_runtime': 0.3215, 'eval_samples_per_second': 311.027, 'eval_steps_per_second': 21.772, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b901b7f431a5463d92d01cd673b3a265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06904372572898865, 'eval_runtime': 0.5101, 'eval_samples_per_second': 196.055, 'eval_steps_per_second': 13.724, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2cef071d284efd849380b6486fc445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.048532530665397644, 'eval_runtime': 0.5119, 'eval_samples_per_second': 195.366, 'eval_steps_per_second': 13.676, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581bbee5a50c4ee8afabefe2988e1b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.049744125455617905, 'eval_runtime': 0.561, 'eval_samples_per_second': 178.253, 'eval_steps_per_second': 12.478, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0118, 'learning_rate': 4.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4f87f4acb6407a8380594720d87b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05985697731375694, 'eval_runtime': 0.35, 'eval_samples_per_second': 285.69, 'eval_steps_per_second': 19.998, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd7372a55304bf194e97c59889de314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05336456745862961, 'eval_runtime': 0.561, 'eval_samples_per_second': 178.254, 'eval_steps_per_second': 12.478, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d16200adf648d8864c0aa26dae5875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05234888195991516, 'eval_runtime': 0.152, 'eval_samples_per_second': 657.879, 'eval_steps_per_second': 46.052, 'epoch': 10.0}\n",
      "{'train_runtime': 436.3893, 'train_samples_per_second': 45.831, 'train_steps_per_second': 2.864, 'train_loss': 0.06708817844390869, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.06708817844390869, metrics={'train_runtime': 436.3893, 'train_samples_per_second': 45.831, 'train_steps_per_second': 2.864, 'train_loss': 0.06708817844390869, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=num_decay,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/ner_rasa_vpv_v2\\config.json\n",
      "Model weights saved in models/ner_rasa_vpv_v2\\pytorch_model.bin\n",
      "tokenizer config file saved in models/ner_rasa_vpv_v2/tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in models/ner_rasa_vpv_v2/tokenizer\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/ner_rasa_vpv_v2/tokenizer\\\\tokenizer_config.json',\n",
       " 'models/ner_rasa_vpv_v2/tokenizer\\\\special_tokens_map.json',\n",
       " 'models/ner_rasa_vpv_v2/tokenizer\\\\vocab.txt',\n",
       " 'models/ner_rasa_vpv_v2/tokenizer\\\\added_tokens.json',\n",
       " 'models/ner_rasa_vpv_v2/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"models/ner_rasa_vpv_v2\")\n",
    "tokenizer.save_pretrained(\"models/ner_rasa_vpv_v2/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.9995116,\n",
       "  'word': 'soft',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.6682099,\n",
       "  'word': '##ing',\n",
       "  'start': 4,\n",
       "  'end': 7},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.99949306,\n",
       "  'word': 'uagates',\n",
       "  'start': 8,\n",
       "  'end': 15},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9995697,\n",
       "  'word': '1',\n",
       "  'start': 16,\n",
       "  'end': 17}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0) # pass device=0 if using gpu\n",
    "pipe(\"\"\"softing uagates 1.73 for wordpress\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.99955875,\n",
       "  'word': 'microsoft',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.95530385,\n",
       "  'word': 'visual c + + 2013 redistribu',\n",
       "  'start': 10,\n",
       "  'end': 36},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9993456,\n",
       "  'word': '12',\n",
       "  'start': 50,\n",
       "  'end': 52},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99857056,\n",
       "  'word': '0',\n",
       "  'start': 53,\n",
       "  'end': 54}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\"\"microsoft visual c++ 2013 redistributable (x64) - 12.0.30501\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.99949896,\n",
       "  'word': 'google',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.99968445,\n",
       "  'word': 'chrome',\n",
       "  'start': 7,\n",
       "  'end': 13},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9995409,\n",
       "  'word': '32',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99672794,\n",
       "  'word': '.',\n",
       "  'start': 16,\n",
       "  'end': 17},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9993685,\n",
       "  'word': '0',\n",
       "  'start': 17,\n",
       "  'end': 18},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99852043,\n",
       "  'word': '1670',\n",
       "  'start': 19,\n",
       "  'end': 23}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"\"\"google chrome 32.0.1670.5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.9875686,\n",
       "  'word': 'cool house technology',\n",
       "  'start': 0,\n",
       "  'end': 21},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.9962328,\n",
       "  'word': 'ewelink',\n",
       "  'start': 22,\n",
       "  'end': 29},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99957496,\n",
       "  'word': '4',\n",
       "  'start': 30,\n",
       "  'end': 31},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9994165,\n",
       "  'word': '3',\n",
       "  'start': 32,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"cool house technology ewelink 4.3.0 for android\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.9991911,\n",
       "  'word': 'fast',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.99947006,\n",
       "  'word': 'fastball',\n",
       "  'start': 21,\n",
       "  'end': 29},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99958426,\n",
       "  'word': '2',\n",
       "  'start': 30,\n",
       "  'end': 31},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99943036,\n",
       "  'word': '5',\n",
       "  'start': 32,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"fastball productions fastball 2.5.3 for joomla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_vendor',\n",
       "  'score': 0.99955875,\n",
       "  'word': 'microsoft',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'cpe_product',\n",
       "  'score': 0.95530385,\n",
       "  'word': 'visual c + + 2013 redistribu',\n",
       "  'start': 10,\n",
       "  'end': 36},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.9993456,\n",
       "  'word': '12',\n",
       "  'start': 50,\n",
       "  'end': 52},\n",
       " {'entity_group': 'cpe_version',\n",
       "  'score': 0.99857056,\n",
       "  'word': '0',\n",
       "  'start': 53,\n",
       "  'end': 54}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipe(\"\"\"Microsoft Visual C++ 2013 Redistributable (x64) - 12.0.30501\"\"\")\n",
    "# pipe(\"\"\"microsoft visual c++ 2013 redistributable (x64) - 12.0.30501\"\"\")\n",
    "\n",
    "# pipe(\"\"\"google chrome 32.0.1670.5\"\"\")\n",
    "# pipe(\"\"\"draw.io 2.6.3 for confluence\"\"\")\n",
    "\n",
    "\n",
    "# pipe(\"\"\"progress sitefinity 9.2\"\"\")\n",
    "# pipe(\"bitnami containers 7.30.1-debian-10-r40 for laravel\")\n",
    "\n",
    "# pipe(\"cool house technology ewelink 4.3.0 for android\")\n",
    "# pipe(\"fastball productions fastball 2.5.3 for joomla\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ner_poc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f67507cd8543ab18e83f8a3a6d5dfd26fb3b380c571e44157f04b507d239be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
